#!/bin/bash
#location of HPL 
HPCG_DIR=`pwd`




module load cuda/10.0.130

export PATH=/home/award/openmpi-3.1.2_with-ucx_without-cuda_without-slurm/bin:$PATH
export LD_LIBRARY_PATH=/home/award/openmpi-3.1.2_with-ucx_without-cuda_without-slurm/lib:/home/award/UCX/ucx-1.3.1_installation/lib:$LD_LIBRARY_PATH

HPCG_BIN=xhpcg-3.1_gcc_485_cuda-10.0.130_ompi-3.1.0_sm_35_sm_50_sm_60_sm_70_sm_75_ver_10_9_18




module list

nvcc --version
mpirun --version

TESTNAME="2-NODE-V100-SXM2-16G"

HOSTNAME=`hostname`
DATETIME=`hostname`.`date +"%m%d.%H%M%S"`

echo HOSTNAME=$HOSTNAME

nvidia-smi

#***********************************

# Note the hpcg.dat file defines the grid size and running time paramters. For an official subimssion you should use the longer (1 hr+) run time. Benchmark result should typically be about same as the shorter (1 min) run time.

cp hpcg.dat_256x256x256_60 hpcg.dat       	# 60 sec run 
#cp hpcg.dat_256x256x256_3660 hpcg.dat    	# 3660 sec run - use for official submission

#export OMP_NUM_THREADS=20
export OMP_NUM_THREADS=1



echo " ****** running HPCG - binary=$HPCG_BIN on 2 NODES, 4 GPUs ***************************** "
mpirun -np 4 --hostfile host2_4 $HPCG_DIR/$HPCG_BIN | tee ./results/xhpcg_4_gpu-$TESTNAME-$DATETIME-output.txt
echo " ****** running HPCG -  binary=$HPCG_BIN on 2 NODES, 8 GPUs ***************************** "
mpirun -np 8 --hostfile host2_8  $HPCG_DIR/$HPCG_BIN | tee ./results/xhpcg_8_gpu-$TESTNAME-$DATETIME-output.txt
echo " ****** running HPCG -  binary=$HPCG_BIN on 2 NODES, 16 GPUs ***************************** "
mpirun -np 16 --hostfile host2_16  $HPCG_DIR/$HPCG_BIN | tee ./results/xhpcg_16_gpu-$TESTNAME-$DATETIME-output.txt


