#!/bin/bash
#location of HPL 
HPCG_DIR=`pwd`



#*************************************************************************************************
# env modules below ere tested on nvidia psg cluster 
#**************************************************************************************************


module load cuda/10.0.130

export PATH=/home/award/openmpi-3.1.2_with-ucx_without-cuda_without-slurm/bin:$PATH
export LD_LIBRARY_PATH=/home/award/openmpi-3.1.2_with-ucx_without-cuda_without-slurm/lib:/home/award/UCX/ucx-1.3.1_installation/lib:$LD_LIBRARY_PATH

HPCG_BIN=xhpcg-3.1_gcc_485_cuda-10.0.130_ompi-3.1.0_sm_35_sm_50_sm_60_sm_70_sm_75_ver_10_9_18

module list

nvcc --version
mpirun --version

#TESTNAME="V100-SXM2-16G"
TESTNAME="K40-12G"


HOSTNAME=`hostname`
DATETIME=`hostname`.`date +"%m%d.%H%M%S"`

echo "Results in ./results/xhpcg_x_gpu-$DATETIME-output.txt"
#********************************************************************
# Examples for running with boosted clocks on P100 
# Uncomment and modifiy appropriate lines as desired
#********************************************************************

nvidia-smi


#sudo nvidia-smi -pm 1
#sudo nvidia-smi -acp 0

#sudo nvidia-smi -ac 715,1328 # max clocks on P100 PCIe
#sudo nvidia-smi -ac 715,1480 # max clocks on P100 SXM2


#***********************************

# Note the hpcg.dat file defines the grid size and running time paramters. For an official subimssion you should use the longer (1 hr+) run time. Benchmark result should typically be about same as the shorter (1 min) run time.


#cp hpcg.dat_128x128x128_60 hpcg.dat
#cp hpcg.dat_128x128x256_60 hpcg.dat
#cp hpcg.dat_128x256x256_60 hpcg.dat

#cp hpcg.dat_256x256x512_60 hpcg.dat
cp hpcg.dat_256x256x256_60 hpcg.dat       	# 60 sec run 
#cp hpcg.dat_256x256x256_3660 hpcg.dat    	# 3660 sec run - use for official submission

#export OMP_NUM_THREADS=20
#export OMP_NUM_THREADS=16
export OMP_NUM_THREADS=1

#MPIFLAGS="--mca btl sm,self --mca btl_openib_warn_default_gid_prefix 0"
#MPIFLAGS="--mca btl vader,self --mca btl_openib_warn_default_gid_prefix 0"
MPIFLAGS=" --mca btl_openib_warn_default_gid_prefix 0"

#cho " ****** running HPCG binary=$HPCG_BIN on 1 GPUs ***************************** "
#mpirun -np 1 $MPIFLAGS $HPCG_DIR/$HPCG_BIN | tee ./results/xhpcg_1_gpu-$DATETIME-output.txt
echo " ****** running HPCG binary=$HPCG_BIN on 2 GPUs ***************************** "
export CUDA_VISIBLE_DEVICES=0,1
#export CUDA_VISIBLE_DEVICES=0,4

mpirun -np 2 $MPIFLAGS $HPCG_DIR/$HPCG_BIN | tee ./results/xhpcg_2_gpu-$TESTNAME-$DATETIME-output.txt
echo " ****** running HPCG binary=$HPCG_BIN on 4 GPUs ***************************** "
export CUDA_VISIBLE_DEVICES=0,1,2,3
#export CUDA_VISIBLE_DEVICES=0,2,4,6

mpirun -np 4 $MPIFLAGS $HPCG_DIR/$HPCG_BIN | tee ./results/xhpcg_4_gpu-$TESTNAME-$DATETIME-output.txt
echo " ****** running HPCG binary=$HPCG_BIN on 8 GPUs ***************************** "

export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
#mpirun -np 8 $MPIFLAGS $HPCG_DIR/$HPCG_BIN | tee ./results/xhpcg_8_gpu-$TESTNAME-$DATETIME-output.txt

echo " ****** 2 GPU result **********************"
grep "final" ./results/xhpcg_2_gpu-$TESTNAME-$DATETIME-output.txt
echo ""
echo " ****** 4 GPU result **********************"
grep "final" ./results/xhpcg_4_gpu-$TESTNAME-$DATETIME-output.txt
echo ""
echo " ****** 8 GPU result **********************"
grep "final" ./results/xhpcg_8_gpu-$TESTNAME-$DATETIME-output.txt
echo ""






